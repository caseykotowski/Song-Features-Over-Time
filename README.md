# Song Features Over Time
This project will take the metadata of tracks from spotify across 100 years to analyze how music has changed over time.

## Purpose

This topic was selected after exploring what data Spotify keeps on the music uploaded to its platform.
Spotify classifies music based on the following features:

* id (Id of track generated by Spotify)
* acousticness (Ranges from 0 to 1)
* danceability (Ranges from 0 to 1)
* energy (Ranges from 0 to 1)
* duration_ms (Integer typically ranging from 200k to 300k)
* instrumentalness (Ranges from 0 to 1)
* valence (Ranges from 0 to 1)
* popularity (Ranges from 0 to 100)
* tempo (Float typically ranging from 50 to 150)
* liveness (Ranges from 0 to 1)
* loudness (Float typically ranging from -60 to 0)
* speechiness (Ranges from 0 to 1)
* mode (0 = Minor, 1 = Major)
* explicit (0 = No explicit content, 1 = Explicit content)
* key (All keys on octave encoded as values ranging from 0 to 11, starting on C as 0, C# as 1 and so onâ€¦)
* timesignature (The predicted timesignature, most typically 4)
* artists (List of artists mentioned)
* artists (Ids of mentioned artists)
* release_date (Date of release mostly in yyyy-mm-dd format, however precision of date may vary)
* name (Name of the song)

We're curious about how the ratio of these features has changed over time. Are any cyclical trends? How does the breakdown look for different genres, and can we find how new genres are created?
 
## Data Source
We are sourcing our data from the following dataset on Kaggle:

[Spotify Dataset 1922-2021](https://www.kaggle.com/yamaerenay/spotify-dataset-19212020-160k-tracks)


## Project Team Communication Protocol 

* Main communication will be through our Slack channel 
* Meet twice weekly via Zoom
    * Monday & Wednesday 7-9pm
* Add additional zoom meetings if needed
    * Thurs-Sun timeframe

## Database
To store our data we are using the S3 app in AWS. We will be uploading all relevant raw CSV files into a bucket for easy and remote access. We are using Amazon RDS Database to store our data post ETL. The DB identifier is spotify-db in RDS:

[Spotify Databse](https://us-east-2.console.aws.amazon.com/rds/home?region=us-east-2#database:id=spotify-db;is-cluster=false;tab=connectivity)

The following image is the logical ERD that will serve as the model for our database:

![ERD.png](Images/ERD.png)

We updated our ERD to simplify the tables we are using. We will only need track and artist information to analyze spotify genre data. The following tables were created:

**track_info**
* id (PK)
* name
* popularity
* duration_ms
* explicit
* artists
* release_date
* danceability
* key
* id_artists (FK)
* energy
* loudness
* mode
* speechiness
* acousticness
* instrumentalness
* liveness
* valence
* tempo
* time_signature

**artist_info**
* id (PK) 
* name
* genres
* popularity

A new ERD was created:

![QuickDBD-Free Diagram](https://user-images.githubusercontent.com/81447450/129500744-0caeabba-8d61-46bf-b458-e975eb8ad57e.png)

To clean up the large data set, the "followers" column was removed. Nulls and duplicates were also removed from the track table. 
To add genres to tracks, a full outer join was performed with the track_info and artist_info in pgAdmin. This can also be done in the colab notebook code to make a permanent join.

See our loading data code:

[mcc_loading_data.ipynb](https://github.com/caseykotowski/Song-Features-Over-Time/blob/1baf83a51f3a9f009a07d9ba279b896105bdd07e/DataBase%20S2/MCC_Loading_Data.ipynb)

<img width="227" alt="Full outer join of 2 tables" src="https://user-images.githubusercontent.com/81447450/129501027-02c417fc-1baa-4107-8a9d-711e3591d9c6.png">
<img width="196" alt="PGAdmin 4 tables" src="https://user-images.githubusercontent.com/81447450/129502526-358ab1f2-eed7-4da7-9d47-a80a50fe3876.png">


Further data clean up will have to be performed either prior to uploading to RDS or on pgAdmin since there are some fields with odd formatting (i.e. dates, genres, artists). Formatting those fields could help with data analysis and visualization.  

## Machine Learning

The unsupervised **hierarchical clustering** is a feasible machine learning algorithm that can be used to breakdown and organize genres in the Spotify Track Database. This algorithm is preferred since it does not require a known cluster number, uses a dendrogram to identify cluster numbers, and is good for grouping data. Down sides of using the hierarchical clustering algorithm is that larger datasets increase run time and the dendrogram could be more confusing than helpful if there are too many clusters.

A dendrogram could be useful to identify the appropriate number of clusters or genres by identifying tracks that are similar based on key characteristics, such as acousticness, danceability, energy, etc.

Example of dendrogram:
![data-18-6-3-3-resulting-dendrogram-three-clusters](https://user-images.githubusercontent.com/81447450/127786442-a1cf0610-42e8-41d3-8205-03db57a3538c.png)

To run the hierarchical clustering algorithm, we can run Agglomerative Clustering from Scikit Learn package and use the cluster number identified in the the dendrogram. 
[Pseudo Code for algorithm](https://github.com/caseykotowski/Song-Features-Over-Time/blob/6e4e8af93799df650a785f97095db41907ec8f09/Pseudo_Code_S1_Machine_Learning.ipynb)

This algorithm can possibly used to predict how a song will be categorized by genre. 

### Data preprocessing 

To begin preprocessing, I used pyspark to load in the data from our AWS database. 
However, I want to use Pandas to work with the data, so I converted the pyspark dataframes
to pandas dataframes. 
Once in Pandas, I merged the tracks and artists dataframes on the name of the artist so we could
see the genres by track. 
I pulled the data types for each column, so I knew what I would need to convert.
Next, I dropped columns that I did not find relevant to the clustering process, which I will discuss further
in the feature selection section. 
The remaining columns were all numeric in nature, but were still in an object datatype, so 
I converted all columns to floats. Machine learning algorithms need numeric columns to properly cluster.
Next, I filtered by popularity over 50. More popular songs seem more likely to be closer to the definition of what their genre is, 
also there is a very large amount of data. In the interest of time, it makes more sense to filter down how much
data we are putting into the model. I chose popularity above 50 because those songs are more popular than not at that point, which
will show us if popular music is closer to the definitions of their respective genres.
Next, I dropped any null columns. We have so much data that losing a few rows will not impact our model.

Finally, I created a random sample dataframe from the spotify df. The final dataframe has too much 
data to run the models with the amount of RAM on my computer. 

### Feature Engineering 

I chose to drop the following columns:
'id', 'name', 'duration_ms', 'explicit', 'artists', 'release_date', 'followers', 'genres', 'id_artists'

I chose to keep the following columns:
'popularity', 'danceability', 'energy', 'loudness', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'key', 'mode', 'time_signature'

The name, artists, and release date columns could not meaningfully be converted to numbers that would effect genre clustering. 
The ID, explicit, followers, and ID_Artists columns, while numeric, are numbers that don't have meaning as a number. Followers is also a number that is similar to popularity, 
but not scaled. 

As I am using two unsupervised machine learning algorithms, so there is no target column. Therefore, I also chose to remove the genre column as the question we are asking
is whether or not our algorthims can predict the genre. 

The remaining columns are all features of the song that directly contribute to the genre, and are all numeric in the same scale. Very minimal scaling needed to be done to these columns.

### Train Test Splits 
 
There is no test train split in unsupervised learning, however I took a random sample of the data frame. I chose the number of rows by trial and error of how
many rows I could include before receiving the lack of RAM error again.

### Model Choice

I tested two methods of machine learning :Dendrograph/hierarchical clustering and K-means clustering. 
The main difference is that for k means, we need to pick a k number of clusters ahead of time, whereas hierarchical show us levels of clusters and we pick what makes sense. 

For the k means, we can utilize the elbow graph to show where the number of clusters increasing no longer causes massive differences in the clusters, but I 
think for music genres, hierarchical clustering makes more sense.

The way that Spotify defines genres can become very specific, with reference to more general genres. With the dendrographs, we can see what the clusters look like 
when we desire the predicitons of more specific clusters versus the more general genres. 

The main limitation of the dendrograph is that it is difficult to view the clusters at the more specific levels. I am also not sure how the models would look if I was
able to use the full data set. 

