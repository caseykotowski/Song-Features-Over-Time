{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine-Learning-Spotify-seg2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lWYxOKOacFG"
      },
      "source": [
        "# Standard code for starting Spark\n",
        "import os\n",
        "# Find the latest version of spark 3.0  from http://www-us.apache.org/dist/spark/ and enter as the spark version\n",
        "# For example:\n",
        "# spark_version = 'spark-3.0.2'\n",
        "spark_version = 'spark-3.<enter version>'\n",
        "os.environ['SPARK_VERSION']=spark_version\n",
        "\n",
        "# Install Spark and Java\n",
        "!apt-get update\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q http://www-us.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "# Set Environment Variables\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\"\n",
        "\n",
        "# Start a SparkSession\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zX9ukBg8awlk"
      },
      "source": [
        "# Standard Spark session\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"CloudETL\").config(\"spark.driver.extraClassPath\",\"/content/postgresql-42.2.9.jar\").getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18Wi5OS-b76t"
      },
      "source": [
        " # Read in data from S3 Buckets\n",
        "from pyspark import SparkFiles\n",
        "url=\"https://<bucket name>.s3.amazonaws.com/[INSERT FILE NAME].csv\"\n",
        "spark.sparkContext.addFile(url)\n",
        "[FILE NAME]_df = spark.read.csv(SparkFiles.get(\"user_data.csv\"), sep=\",\", header=True, inferSchema=True)\n",
        "\n",
        "# Show DataFrame\n",
        "[FILE NAME]df.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eIbv4xkgmZa"
      },
      "source": [
        "# Convert Spark DataFrame to Pandas DataFrame\n",
        "dataframe = pd.DataFrame([FILE NAME]df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4D-sDsV6q_6"
      },
      "source": [
        "# Does the database merge the track and artist tables?\n",
        "# If not, load in and merge artists"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DARQAHusliUA"
      },
      "source": [
        "# Import dependencies for Machine Learning\n",
        " import matplotlib.pyplot as plt\n",
        "from scipy.cluster.hierarchy import fcluster, linkage, dendrogram\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import hvplot.pandas\n",
        "from path import Path\n",
        "import plotly.express as px\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWPBUinEqEsK"
      },
      "source": [
        "# Exploratory Data to understand how we need to preprocess"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nA_5vrMcuf3v"
      },
      "source": [
        "# Preprocessing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3lVJONAzmj7"
      },
      "source": [
        "# Use Hierarchical clustering\n",
        "# Read data from DB using PySpark\n",
        "# Load in Track.csv from S3 into a DataFrame\n",
        "\n",
        "# Normalize data\n",
        "from sklearn.preprocessing import normalize\n",
        "data_scaled = normalize(data)\n",
        "data_scaled = pd.DataFrame(data_scaled, columns=data.columns)\n",
        "data_scaled.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-tvzYCS72XC"
      },
      "source": [
        "\n",
        "# Create hierarchal clustering array using n_clusters determined from dendrogram analysis\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "cluster = AgglomerativeClustering(n_clusters=X, affinity='euclidean', linkage='ward')  \n",
        "cluster.fit_predict(data_scaled)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbUGCpj67_Vc"
      },
      "source": [
        "# Plot scatter plot of clusters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_kuF-HE8As3"
      },
      "source": [
        "# Trying the next machine learning model\n",
        "# Which one shall I pick? "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}