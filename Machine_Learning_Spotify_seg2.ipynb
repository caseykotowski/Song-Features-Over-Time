{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine-Learning-Spotify-seg2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lWYxOKOacFG"
      },
      "source": [
        "import os\n",
        "# Find the latest version of spark 3.0  from http://www.apache.org/dist/spark/ and enter as the spark version\n",
        "# For example:\n",
        "# spark_version = 'spark-3.0.3'\n",
        "spark_version = 'spark-3.1.2'\n",
        "os.environ['SPARK_VERSION']=spark_version\n",
        "\n",
        "# Install Spark and Java\n",
        "!apt-get update\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "# Set Environment Variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\"\n",
        "\n",
        "# Start a SparkSession\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zX9ukBg8awlk"
      },
      "source": [
        "# Start Spark session\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"LoadingData\").getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18Wi5OS-b76t"
      },
      "source": [
        "# Read in data from S3 Buckets\n",
        "from pyspark import SparkFiles\n",
        "url = \"https://spot-i-fy-bucket.s3.us-east-2.amazonaws.com/tracks.csv\"\n",
        "spark.sparkContext.addFile(url)\n",
        "df = spark.read.csv(SparkFiles.get(\"tracks.csv\"), sep=\",\", header=True)\n",
        "\n",
        "# Show DataFrame\n",
        "df.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDnZyvrSvbcA"
      },
      "source": [
        "# Print our schema\n",
        "df.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4D-sDsV6q_6"
      },
      "source": [
        "# Read in data from S3 Buckets\n",
        "from pyspark import SparkFiles\n",
        "url = \"https://spot-i-fy-bucket.s3.us-east-2.amazonaws.com/artists.csv\"\n",
        "spark.sparkContext.addFile(url)\n",
        "a_df = spark.read.csv(SparkFiles.get(\"artists.csv\"), sep=\",\", header=True)\n",
        "\n",
        "# Show DataFrame\n",
        "a_df.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DARQAHusliUA"
      },
      "source": [
        "# Import dependencies for Machine Learning\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.cluster.hierarchy import fcluster, linkage, dendrogram\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import hvplot.pandas\n",
        "from path import Path\n",
        "import plotly.express as px\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.cluster import KMeans"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AIrKDtN5jOb"
      },
      "source": [
        "# Convert Spark DataFrame to Pandas DataFrame\n",
        "artists_df = pd.DataFrame(a_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vgJZAdx5tOl"
      },
      "source": [
        "# Convert Spark DataFrame to Pandas DataFrame\n",
        "track_df = pd.DataFrame(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3hb198l2QAD"
      },
      "source": [
        "# Merge the two dataframes\n",
        "# How to merge when columns have different name\n",
        "music_df = track_df.merge(artists_df, left=['artists', 'name'])\n",
        "music_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWPBUinEqEsK"
      },
      "source": [
        "# Exploratory Data to understand how we need to preprocess\n",
        "# Explore popularity range - over 75? over 50?\n",
        "music_df.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPitpLSF8Isd"
      },
      "source": [
        "# Drop columns that won't be useful to clustering by genre\n",
        "# Is it okay to not merge for Machine Learning, but just for visualizations? The artists df gives us the answer\n",
        "# Drop id, name, popularity, duration_ms, explicit, artists, id_artists, release_date, \n",
        "spotify_df = music_df.drop(columns=['id', 'name', 'popularity', 'duration_ms', 'explicit', 'artists', ''])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nA_5vrMcuf3v"
      },
      "source": [
        "# Preprocessing\n",
        "# US only\n",
        "# Popularity? What's a good cut off?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QE80W3JI_4PB"
      },
      "source": [
        "# Feature selection: "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3lVJONAzmj7"
      },
      "source": [
        "# Use Hierarchical clustering\n",
        "# Read data from DB using PySpark\n",
        "# Load in Track.csv from S3 into a DataFrame\n",
        "\n",
        "# Normalize data\n",
        "from sklearn.preprocessing import normalize\n",
        "data_scaled = normalize(data)\n",
        "data_scaled = pd.DataFrame(data_scaled, columns=data.columns)\n",
        "data_scaled.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-tvzYCS72XC"
      },
      "source": [
        "\n",
        "# Create hierarchal clustering array using n_clusters determined from dendrogram analysis\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "cluster = AgglomerativeClustering(n_clusters=X, affinity='euclidean', linkage='ward')  \n",
        "cluster.fit_predict(data_scaled)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbUGCpj67_Vc"
      },
      "source": [
        "# Plot scatter plot of clusters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iG6WlYk0AB4V"
      },
      "source": [
        "# Does using PCA help? Some clusters are opposite/inverse of others, so it might hurt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xyw1yUADBPUC"
      },
      "source": [
        "# Go through a few models. 3 maybe? Why do I think the best performing model is best performing?\n",
        "\n",
        "# Notes from class! Is this data set actually labeled? Maybe I want to see if this can predict genre?\n",
        "# How would I design a neural network? Use that loop to pick the number of features?\n",
        "# How accurate/good is each model?\n",
        "# Were we able to use a simple model?\n",
        "# Do we get similar results each time? "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeBb4eUvrzS_"
      },
      "source": [
        "# I'll do k means clustering and round it out with a neural network\n",
        "# Question is, can we predict the genre lable?\n",
        "# Talk about stuff over time for the tableau bit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBICku2h6FR5"
      },
      "source": [
        "#K Means Clustering\n",
        "#Begin with elbow graph to determine how many clusters we might want\n",
        "inertia = []\n",
        "k = list(range(1, 11))\n",
        "# Calculate the inertia for the range of K values\n",
        "for i in k:\n",
        "   km = KMeans(n_clusters=i, random_state=0)\n",
        "   km.fit(df_shopping)\n",
        "   inertia.append(km.inertia_)\n",
        "#Create elbow curve in hvPlot\n",
        "elbow_data = {\"k\":k, \"inertia\":inertia}\n",
        "df_elbow = pd.DataFrame(elbow_data)\n",
        "df_elbow.hvplot.line(x-\"k\", y=\"inertia\", xticks=k, title=\"Elbow Curve\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2RlStjBhZjH"
      },
      "source": [
        "# Set X \n",
        "X = spotify_df\n",
        "X_scaled = StandardScaler().fit_transform(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVJVvY8yBWFS"
      },
      "source": [
        "# Create K Means Model with X clusters (from elbow plot)\n",
        "model = KMeans(n_clusters=3, random_state=42).fit(X_scaled)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9C7m4qhBk_J"
      },
      "source": [
        "# Calculate predicted values.\n",
        "y_pred = model.predict(X_scaled)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dl5elaKvBrlQ"
      },
      "source": [
        "# Add predicted values onto the original dataframe\n",
        "df_y = pd.DataFrame(y_pred, columns=['Cluster'])\n",
        "combined = df.join(df_y, how='inner')\n",
        "combined.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLticF67xDee"
      },
      "source": [
        "# Plot clusters\n",
        "# Create a scatterplot of df_iris\n",
        "#df_iris.hvplot.scatter(x=\"SOME COLUMN\", y=\"SOME COLUMN\", by=\"SOME COLUMN\")\n",
        "fig.update_layout(legend=dict(x=0,y=1))\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNETqpqIi1Xz"
      },
      "source": [
        "#Neural Network\n",
        "# Import our dependencies\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler,OneHotEncoder\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqAM0YSOkFmm"
      },
      "source": [
        "# Use the preprocessed data from the previous \n",
        "# Wait can I even use a normal neural network? \n",
        "# https://en.wikipedia.org/wiki/Self-organizing_map"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}